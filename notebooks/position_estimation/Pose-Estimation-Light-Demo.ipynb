{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from models.with_mobilenet import PoseEstimationWithMobileNet\n",
    "from modules.keypoints import extract_keypoints, group_keypoints\n",
    "from modules.load_state import load_state\n",
    "from modules.pose import Pose, track_poses\n",
    "from val import normalize, pad_width\n",
    "\n",
    "from modules.one_euro_filter import OneEuroFilter\n",
    "from modules.keypoints import BODY_PARTS_KPT_IDS, BODY_PARTS_PAF_IDS\n",
    "\n",
    "import matplotlib as plt\n",
    "from matplotlib import cm\n",
    "from modules.pose import Pose\n",
    "\n",
    "from demo import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('model/pretrained.pth', <http.client.HTTPMessage at 0x7f9da0824668>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib\n",
    "urllib.request.urlretrieve (\"https://download.01.org/opencv/openvino_training_extensions/models/human_pose_estimation/checkpoint_iter_370000.pth\",\n",
    "                    \"model/pretrained.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pose may contain up to 18 keypoints: ears, eyes, nose, neck, shoulders, elbows, wrists, hips, knees, and ankles.\n",
    "\n",
    "For  multi-person  pose  es-timation,  most  approaches have  used  a  top-down  strategy  that  firstdetects  people  and  then  have  estimated  the  pose  of  eachperson  independently  on  each  detected  region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/openpose_pipeline.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/openpose-body-architecture-1024x291.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('images/joker.jpeg', cv2.IMREAD_COLOR)\n",
    "img_disp = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = PoseEstimationWithMobileNet()\n",
    "checkpoint = torch.load('model/pretrained.pth', map_location='cpu')\n",
    "load_state(net, checkpoint)\n",
    "net = net.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_disp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = 8\n",
    "upsample_ratio = 8\n",
    "num_keypoints = Pose.num_kpts\n",
    "orig_img = img.copy()\n",
    "\n",
    "heatmaps, pafs, scale, pad = infer_fast(net, img, 512, stride, upsample_ratio, cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 10])\n",
    "plt.imshow(np.max(heatmaps[:,:,:18], axis=2))\n",
    "plt.imshow(cv2.resize(img_disp, (776,512)), alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 10])\n",
    "plt.imshow(1 - pafs[:,:,2])\n",
    "plt.imshow(cv2.resize(img_disp, (776,512)), alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 10])\n",
    "plt.imshow(np.abs(pafs[:,:,14]))\n",
    "plt.imshow(cv2.resize(img_disp, (776,512)), alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip=(slice(None,None,2),slice(None,None,2))\n",
    "plt.figure(figsize=[15, 10])\n",
    "plt.quiver(pafs[160:220, 320:420, 14][skip], - pafs[160:220, 320:420, 15][skip], color='cyan',scale=30)\n",
    "plt.imshow(cv2.resize(img_disp, (776,512))[160:220, 320:420], extent=(-0.5, 50-0.5, 30-0.5, -0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/keypoint_extraction.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_keypoints_num = 0\n",
    "all_keypoints_by_type = []\n",
    "for kpt_idx in range(num_keypoints):  # 19th for bg\n",
    "    total_keypoints_num += extract_keypoints(heatmaps[:, :, kpt_idx], all_keypoints_by_type, total_keypoints_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/keypoints.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints = {\n",
    "        0: \"nose\",\n",
    "        1: \"neck\",\n",
    "        2: \"left_shoulder\",\n",
    "        3: \"left_elbow\",\n",
    "        4: \"left_wrist\",\n",
    "        5: \"right_shoulder\",\n",
    "        6: \"right_elbow\",\n",
    "        7: \"right_wrist\",\n",
    "        8: \"left_hip\",\n",
    "        9: \"left_knee\",\n",
    "        10: \"left_ankle\",\n",
    "        11: \"right_hip\",\n",
    "        12: \"right_knee\",\n",
    "        13: \"right_ankle\",\n",
    "        14: \"left_eye\",\n",
    "        15: \"right_eye\",\n",
    "        16: \"left_ear\",\n",
    "        17: \"right_ear\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f'{len(x)} keypoints extracted for {keypoints[i]}' for i, x in enumerate(all_keypoints_by_type)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too many point detected for ankles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_with_ankle_keypoints = cv2.resize(img_disp.copy(), (776,512))\n",
    "for ankle_keypoint in all_keypoints_by_type[10]:\n",
    "    img_with_ankle_keypoints = cv2.circle(img_with_ankle_keypoints,\n",
    "                                          ankle_keypoint[:2], 10, color=(0, 255, 255), thickness=-1)\n",
    "    \n",
    "plt.imshow(img_with_ankle_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positions(all_keypoints_by_type, pafs):\n",
    "    pose_entries, all_keypoints = group_keypoints(all_keypoints_by_type, pafs, demo=True)\n",
    "    for kpt_id in range(all_keypoints.shape[0]):\n",
    "        all_keypoints[kpt_id, 0] = (all_keypoints[kpt_id, 0] * stride / upsample_ratio - pad[1]) / scale\n",
    "        all_keypoints[kpt_id, 1] = (all_keypoints[kpt_id, 1] * stride / upsample_ratio - pad[0]) / scale\n",
    "    current_poses = []\n",
    "    for n in range(len(pose_entries)):\n",
    "        if len(pose_entries[n]) == 0:\n",
    "            continue\n",
    "        pose_keypoints = np.ones((num_keypoints, 2), dtype=np.int32) * -1\n",
    "        for kpt_id in range(num_keypoints):\n",
    "            if pose_entries[n][kpt_id] != -1.0:  # keypoint was found\n",
    "                pose_keypoints[kpt_id, 0] = int(all_keypoints[int(pose_entries[n][kpt_id]), 0])\n",
    "                pose_keypoints[kpt_id, 1] = int(all_keypoints[int(pose_entries[n][kpt_id]), 1])\n",
    "        pose = Pose(pose_keypoints, pose_entries[n][18])\n",
    "        current_poses.append(pose)\n",
    "        \n",
    "    return current_poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positions = get_positions(all_keypoints_by_type, pafs)\n",
    "colormap = cm.tab10.colors\n",
    "\n",
    "for i, pose in enumerate(all_positions):\n",
    "    pose.draw(img_disp, np.array(colormap[i % len(colormap)])*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10, 10])\n",
    "plt.imshow(img_disp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
