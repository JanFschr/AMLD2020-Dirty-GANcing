{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose**: This notebook provides a walk through the process of estimating the position of individuals on an image with a pretrained lightweight OpenPose model proposed in (https://arxiv.org/pdf/1811.12004.pdf). For a more detailed explanation of the OpenPose pipeline, please refer to the original paper (https://arxiv.org/pdf/1812.08008.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib\n",
    "import os\n",
    "\n",
    "from models.with_mobilenet import PoseEstimationWithMobileNet\n",
    "from modules.keypoints import extract_keypoints, group_keypoints\n",
    "from modules.load_state import load_state\n",
    "from modules.pose import Pose, track_poses\n",
    "from modules.one_euro_filter import OneEuroFilter\n",
    "from modules.keypoints import BODY_PARTS_KPT_IDS, BODY_PARTS_PAF_IDS\n",
    "from val import normalize, pad_width\n",
    "from matplotlib import cm\n",
    "from modules.pose import Pose\n",
    "from demo import extract_keypoints, group_keypoints, infer_fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pose Estimation task aims to determine the position of the different body parts of individuals that can be found on an image.\n",
    "\n",
    "While most of the Multi-Person Pose Estimation approaches have used a top-down strategy, that first detects where the persons are located and then estimates the pose of them at each location, the OpenPose model proposes to solve the task with a bottom-up approach.\n",
    "\n",
    "A multi-stage CNN outputs confidence maps for body parts locations and vector fields that show the probable locations of body parts connections (limbs). These vector fields are called Part Affinity Fields (PAFs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/openpose_pipeline.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OpenPose pipeline first performs a feature extraction step by feeding the image into a Convolutional Neural Network (CNN), generating a set of feature maps **F**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/openpose-body-architecture-1024x291.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('model'):\n",
    "    os.mkdir('model')\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://download.01.org/opencv/\" +\\\n",
    "    \"openvino_training_extensions/models/human_pose_estimation/checkpoint_iter_370000.pth\",\n",
    "                    \"model/pretrained.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = PoseEstimationWithMobileNet()\n",
    "checkpoint = torch.load('model/pretrained.pth', map_location='cpu')\n",
    "load_state(net, checkpoint)\n",
    "net = net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('images/joker.jpeg', cv2.IMREAD_COLOR)\n",
    "img_disp = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.imshow(img_disp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = 8\n",
    "upsample_ratio = 8\n",
    "num_keypoints = Pose.num_kpts\n",
    "orig_img = img.copy()\n",
    "orig_img_shape = img_disp.shape[1], img_disp.shape[0]\n",
    "\n",
    "heatmaps, pafs, scale, pad = infer_fast(net, img, 512, stride, upsample_ratio, cpu=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 10])\n",
    "plt.imshow(cv2.resize(np.max(heatmaps[:,:,:18], axis=2), orig_img_shape))\n",
    "plt.imshow(img_disp, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 10])\n",
    "plt.imshow(cv2.resize(np.abs(pafs[:,:,2]), orig_img_shape))\n",
    "plt.imshow(img_disp, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 10])\n",
    "plt.imshow(cv2.resize(np.abs(pafs[:,:,14]), orig_img_shape))\n",
    "plt.imshow(img_disp, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip=(slice(None,None,2),slice(None,None,2))\n",
    "plt.figure(figsize=[15, 10])\n",
    "# pafs_right_arm_vector_field_x = cv2.resize(pafs[:, :, 14][skip], orig_img_shape)\n",
    "# pafs_right_arm_vector_field_y = cv2.resize(-pafs[:, :, 15][skip], orig_img_shape)\n",
    "\n",
    "\n",
    "plt.quiver(pafs[160:220, 320:420, 14][skip], - pafs[160:220, 320:420, 15][skip], color='cyan',scale=30)\n",
    "plt.imshow(cv2.resize(img_disp, (776,512))[160:220, 320:420], extent=(-0.5, 50-0.5, 30-0.5, -0.5))\n",
    "\n",
    "# plt.quiver(pafs_right_arm_vector_field_x[140:160, 250:350],\n",
    "#            pafs_right_arm_vector_field_y[140:160, 250:350], color='cyan',scale=30)\n",
    "# plt.imshow(img_disp[160:220, 320:420], extent=(-0.5, 50-0.5, 30-0.5, -0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/keypoint_extraction.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_keypoints_num = 0\n",
    "all_keypoints_by_type = []\n",
    "for kpt_idx in range(num_keypoints):  # 19th for bg\n",
    "    total_keypoints_num += extract_keypoints(heatmaps[:, :, kpt_idx], all_keypoints_by_type, total_keypoints_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/keypoints.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints = {\n",
    "        0: \"nose\",\n",
    "        1: \"neck\",\n",
    "        2: \"left_shoulder\",\n",
    "        3: \"left_elbow\",\n",
    "        4: \"left_wrist\",\n",
    "        5: \"right_shoulder\",\n",
    "        6: \"right_elbow\",\n",
    "        7: \"right_wrist\",\n",
    "        8: \"left_hip\",\n",
    "        9: \"left_knee\",\n",
    "        10: \"left_ankle\",\n",
    "        11: \"right_hip\",\n",
    "        12: \"right_knee\",\n",
    "        13: \"right_ankle\",\n",
    "        14: \"left_eye\",\n",
    "        15: \"right_eye\",\n",
    "        16: \"left_ear\",\n",
    "        17: \"right_ear\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f'{len(x)} keypoints extracted for {keypoints[i]}' for i, x in enumerate(all_keypoints_by_type)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too many point detected for ankles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_with_ankle_keypoints = cv2.resize(img_disp.copy(), (776,512))\n",
    "for ankle_keypoint in all_keypoints_by_type[10]:\n",
    "    img_with_ankle_keypoints = cv2.circle(img_with_ankle_keypoints,\n",
    "                                          ankle_keypoint[:2], 10, color=(0, 255, 255), thickness=-1)\n",
    "    \n",
    "plt.imshow(img_with_ankle_keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positions(all_keypoints_by_type, pafs):\n",
    "    pose_entries, all_keypoints = group_keypoints(all_keypoints_by_type, pafs, demo=True)\n",
    "    for kpt_id in range(all_keypoints.shape[0]):\n",
    "        all_keypoints[kpt_id, 0] = (all_keypoints[kpt_id, 0] * stride / upsample_ratio - pad[1]) / scale\n",
    "        all_keypoints[kpt_id, 1] = (all_keypoints[kpt_id, 1] * stride / upsample_ratio - pad[0]) / scale\n",
    "    current_poses = []\n",
    "    for n in range(len(pose_entries)):\n",
    "        if len(pose_entries[n]) == 0:\n",
    "            continue\n",
    "        pose_keypoints = np.ones((num_keypoints, 2), dtype=np.int32) * -1\n",
    "        for kpt_id in range(num_keypoints):\n",
    "            if pose_entries[n][kpt_id] != -1.0:  # keypoint was found\n",
    "                pose_keypoints[kpt_id, 0] = int(all_keypoints[int(pose_entries[n][kpt_id]), 0])\n",
    "                pose_keypoints[kpt_id, 1] = int(all_keypoints[int(pose_entries[n][kpt_id]), 1])\n",
    "        pose = Pose(pose_keypoints, pose_entries[n][18])\n",
    "        current_poses.append(pose)\n",
    "        \n",
    "    return current_poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positions = get_positions(all_keypoints_by_type, pafs)\n",
    "colormap = cm.tab10.colors\n",
    "\n",
    "for i, pose in enumerate(all_positions):\n",
    "    pose.draw(img_disp, np.array(colormap[i % len(colormap)])*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[10, 10])\n",
    "plt.imshow(img_disp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
