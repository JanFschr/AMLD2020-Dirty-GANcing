{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Teaching Objective\n",
    "\n",
    "The aim of this notebook is to show the participants how the ***GAN loss*** are computed and what are the ***inputs*** of the models which implicitely affect the losses. \n",
    "\n",
    "Compared to the other notebooks we are introducing VGG and Feature Matching Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis of original function\n",
    "\n",
    "In the forward pass the different losses used for the training are defined. The most important components of the forward pass are the following:\n",
    "\n",
    "Networks to be trained to generate and discriminate\n",
    "1. ``self.netG``: the generator network\n",
    "2. ``self.netD``: the discriminator network\n",
    "\n",
    "Loss Discriminator\n",
    "1. ``loss_D_fake``, discriminator loss from discriminating fake generated images: -log(1-D(G(z)))\n",
    "2. ``loss_D_real``, discriminator loss from discriminating real images sampled from data: -log(D(x))\n",
    "\n",
    "Loss Generator\n",
    "1. ``loss_G_Gan``, generator loss from the \"Fake Pass Loss\" which label the fake images as correct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Original Function***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, label, inst, image, feat, infer=False):\n",
    "    # Encode Inputs\n",
    "    input_label, inst_map, real_image, feat_map = self.encode_input(label, inst, image, feat)  \n",
    "\n",
    "    # Fake Generation\n",
    "    if self.use_features:\n",
    "        if not self.opt.load_features:\n",
    "            feat_map = self.netE.forward(real_image, inst_map)                     \n",
    "        input_concat = torch.cat((input_label, feat_map), dim=1)                        \n",
    "    else:\n",
    "        input_concat = input_label\n",
    "    # TODO----------------------#    \n",
    "    fake_image = self.netG.forward(input_concat.float())\n",
    "\n",
    "    # Fake Detection and Loss\n",
    "    pred_fake_pool = self.discriminate(input_label, fake_image, use_pool=True)\n",
    "    loss_D_fake = self.criterionGAN(pred_fake_pool, False)        \n",
    "\n",
    "    # Real Detection and Loss        \n",
    "    pred_real = self.discriminate(input_label, real_image)\n",
    "    loss_D_real = self.criterionGAN(pred_real, True)\n",
    "\n",
    "    # GAN loss (Fake Passability Loss)        \n",
    "    pred_fake = self.netD.forward(torch.cat((input_label, fake_image), dim=1))        \n",
    "    loss_G_GAN = self.criterionGAN(pred_fake, True)               \n",
    "\n",
    "    # GAN feature matching loss\n",
    "    loss_G_GAN_Feat = 0\n",
    "    if not self.opt.no_ganFeat_loss:\n",
    "        feat_weights = 4.0 / (self.opt.n_layers_D + 1)\n",
    "        D_weights = 1.0 / self.opt.num_D\n",
    "        for i in range(self.opt.num_D):\n",
    "            for j in range(len(pred_fake[i])-1):\n",
    "                loss_G_GAN_Feat += D_weights * feat_weights * \\\n",
    "                    self.criterionFeat(pred_fake[i][j], pred_real[i][j].detach()) * self.opt.lambda_feat\n",
    "\n",
    "    # VGG feature matching loss\n",
    "    loss_G_VGG = 0\n",
    "    if not self.opt.no_vgg_loss:\n",
    "        loss_G_VGG = self.criterionVGG(fake_image, real_image) * self.opt.lambda_feat\n",
    "\n",
    "    # Only return the fake_B image if necessary to save BW\n",
    "    return [ self.loss_filter( loss_G_GAN, loss_G_GAN_Feat, loss_G_VGG, loss_D_real, loss_D_fake ), None if not infer else fake_image ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the notebook of easier comprehension, the functions have been simplified and commented.\n",
    "\n",
    "``forward`` function modifications:\n",
    "\n",
    "-``netE``: not in use\n",
    "\n",
    "-``nwtG`` and ``netD`` become functions themselves which are initialized before\n",
    "\n",
    "-``self.encode_input`` become a separate function to explain what goes into the model and is placed outside the forward to teach the audience what goes in the forward path and needs to be encoded. The encoding part itself is probably irrelevant, but a independent function has to be created if an example of forward pass has to be created.\n",
    "\n",
    "The forward pass is the most interesting one, here we can show it detached from the class itself, and then when we show the training loop, we highlight where the model calls it.\n",
    "\n",
    "\n",
    "\n",
    "*Ideas to clean up the function more*:\n",
    "\n",
    "-``criterionGAN`` becomes a function\n",
    "\n",
    "-condition of training the gan based on the generator and discriminator features' neurons.\n",
    "\n",
    "-condition of training the gan using VGG to recognize images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "\n",
    "\n",
    "1) Maybe further the functions based on the input parameter of ``opt``.\n",
    "\n",
    "2) Load the poses and images\n",
    "\n",
    "3) Create the cell where the network's inference ability is shown at different training phases.\n",
    "\n",
    "3) Simplify function for criterionGAN\n",
    "\n",
    "4) Simplify criterionFeat and criterionVGG\n",
    "\n",
    "5) Double check with Gaetan and Thibault about content and what to be added and learn most important opt parameters.\n",
    "\n",
    "Qs: \n",
    "\n",
    "- Is the explaination of the input dimension correct?\n",
    "- Does the encode_input add noise to label_map?\n",
    "- Weights in VGG loss and loop\n",
    "- Feature matching loss part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***All below is the actual notebook which will be used in the workshop***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary package imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from src.pix2pixHD.util.image_pool import ImagePool\n",
    "from src.pix2pixHD.models.base_model import BaseModel\n",
    "from src.pix2pixHD.models import networks\n",
    "import src.config.train_opt_notebook as opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass Pix2Pix\n",
    "\n",
    "The aim of this notebook is to show how the ***GAN loss*** are computed and what are the ***inputs*** of the models involved in the training. \n",
    "\n",
    "VGG and Feature Matching Loss are introduced and their function is explained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the generator and the discriminator model are initialized since they are going to be used in the training process.\n",
    "\n",
    "Among the config variables which are used to define the network, some which are worth noticing are:\n",
    "\n",
    "- ``netG_input_nc`` is the number of classes which are fed in the generator model. In other words it is the number of coordinates to define a pose.\n",
    "- ``use_sigmoid`` is the option which being set to *True* defines the use of the least square loss.\n",
    "- ``netD_input_nc`` is the number of input classes going into the generator which are the classes defining a pose and those defining the image.\n",
    "\n",
    "As it can be seen the structure of the generator and the discriminator is extremely complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GlobalGenerator(\n",
      "  (model): Sequential(\n",
      "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (1): Conv2d(18, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (3): ReLU(inplace)\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (6): ReLU(inplace)\n",
      "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (9): ReLU(inplace)\n",
      "    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (11): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (12): ReLU(inplace)\n",
      "    (13): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (14): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (15): ReLU(inplace)\n",
      "    (16): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (17): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (18): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (19): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (20): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (21): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (22): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (23): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (24): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (25): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (26): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (27): ReLU(inplace)\n",
      "    (28): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (29): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (30): ReLU(inplace)\n",
      "    (31): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (32): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (33): ReLU(inplace)\n",
      "    (34): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (35): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (36): ReLU(inplace)\n",
      "    (37): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (38): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (39): Tanh()\n",
      "  )\n",
      ")\n",
      "MultiscaleDiscriminator(\n",
      "  (scale0_layer0): Sequential(\n",
      "    (0): Conv2d(21, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
      "  )\n",
      "  (scale0_layer1): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
      "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace)\n",
      "  )\n",
      "  (scale0_layer2): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
      "    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace)\n",
      "  )\n",
      "  (scale0_layer3): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
      "    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace)\n",
      "  )\n",
      "  (scale0_layer4): Sequential(\n",
      "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
      "  )\n",
      "  (scale1_layer0): Sequential(\n",
      "    (0): Conv2d(21, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
      "  )\n",
      "  (scale1_layer1): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
      "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace)\n",
      "  )\n",
      "  (scale1_layer2): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
      "    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace)\n",
      "  )\n",
      "  (scale1_layer3): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
      "    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace)\n",
      "  )\n",
      "  (scale1_layer4): Sequential(\n",
      "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
      "  )\n",
      "  (downsample): AvgPool2d(kernel_size=3, stride=2, padding=[1, 1])\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "netG_input_nc = opt.label_nc\n",
    "generator_model=networks.define_G(netG_input_nc, opt.output_nc, opt.ngf, opt.netG, \n",
    "                                      opt.n_downsample_global, opt.n_blocks_global, opt.n_local_enhancers, \n",
    "                                      opt.n_blocks_local, opt.norm, gpu_ids=opt.gpu_ids)   \n",
    "use_sigmoid = opt.no_lsgan\n",
    "netD_input_nc = netG_input_nc + opt.output_nc\n",
    "discriminator_model=networks.define_D(netD_input_nc, opt.ndf, opt.n_layers_D, opt.norm, use_sigmoid, \n",
    "                                          opt.num_D, not opt.no_ganFeat_loss, gpu_ids=opt.gpu_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs\n",
    "\n",
    "(This part might be removed) In order to feed the data to the network, the input must be encoded in tensors in order to make them usable for the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input(label_map, real_image, infer=False): \n",
    "    \n",
    "    # create one-hot vector for the pose\n",
    "    size = label_map.size()\n",
    "    oneHot_size = (size[0], opt.label_nc, size[2], size[3])\n",
    "    input_label = torch.FloatTensor(torch.Size(oneHot_size)).zero_().to(self.device)\n",
    "    input_label = input_label.scatter_(1, label_map.data.long().to(self.device), 1.0)\n",
    "    if opt.data_type == 16:\n",
    "        input_label = input_label.half()\n",
    "        \n",
    "    input_label = Variable(input_label, volatile=infer)\n",
    "\n",
    "    # real images for training\n",
    "    if real_image is not None:\n",
    "        real_image = Variable(real_image.data.to(self.device))\n",
    "\n",
    "\n",
    "    return input_label, real_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the GAN's inference ability, a pose is loaded in order to show how its ability to generate dancing individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-87c887113609>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-87c887113609>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    label = # input the label\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "label = # load the label (The poses) \n",
    "image = # load the image (The real images)\n",
    "\n",
    "input_label, real_image = encode_input(label, image)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before exploring the forward pass, it is useful to inspect how the losses are calculated. Hence the functions to compute the LGAN loss, the VGG loss and the Feature Matching loss are reported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GAN_loss():\n",
    "    pass\n",
    "\n",
    "\n",
    "def VGG_loss(x,y):\n",
    "    \n",
    "    #loss criteria\n",
    "    criterion = nn.L1Loss()\n",
    "    \n",
    "    weights = [1.0/32, 1.0/16, 1.0/8, 1.0/4, 1.0]\n",
    "    x_vgg, y_vgg = networks.Vgg19(x,y)\n",
    "    \n",
    "    loss = 0\n",
    "    for i in range(len(x_vgg)):\n",
    "        loss += weights[i] * criterion(x_vgg[i], y_vgg[i].detach())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def Feat_Match_loss():\n",
    "    \n",
    "    criterion=torch.nn.L1Loss()\n",
    "    \n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By going through the forward pass, the participants will get acquainted with the loss involved in the GAN and they will understand how the input previously encoded are used to generate or discriminate fake and real images. Additionally the Feature Matching Loss and VGG Loss will be presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-adefe7e8e0ce>, line 55)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-adefe7e8e0ce>\"\u001b[0;36m, line \u001b[0;32m55\u001b[0m\n\u001b[0;31m    input1, input2 = # ...... the inputs of to calculate the VGG loss using the L1 function to ensure VGG classifies fake and real images equally .........\u001b[0m\n\u001b[0m                                                                                                                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def forward(label, image, feat, infer=False):\n",
    "\n",
    "    input_concat = label\n",
    "    \n",
    "    ### Fake Generation\n",
    "    fake_image = generator_model.forward(input_concat.float())\n",
    "\n",
    "    ### Fake Detection\n",
    "    \n",
    "    #concatenate the condition input_label used to generate the image and the fake image \n",
    "    input_concat = torch.cat((input_label, fake_image.detach()), dim=1) \n",
    "    \n",
    "    #adapt the image dimension\n",
    "    fake_query = ImagePool(opt.pool_size).query(input_concat)\n",
    "        \n",
    "    #predict if the generate image is False or True\n",
    "    pred_fake_pool = discriminator_model.forward(fake_query)\n",
    "    \n",
    "    #predict if the real images is False or True      \n",
    "    pred_real = discriminator_model.forward(input_label, real_image)\n",
    "    \n",
    "    ### Dicriminator Loss Based on Fake Images\n",
    "    \n",
    "    # label_loss=  .....the ground truth to compute the discriminator loss for fake images......\n",
    "    \n",
    "    loss_D_fake = self.criterionGAN(pred_fake_pool, label_loss)      \n",
    "    \n",
    "    ### Dicriminator Loss Based on Fake Images\n",
    "    \n",
    "    # label_loss=  .....the ground truth to compute the discriminator loss for real images......\n",
    "    \n",
    "    loss_D_real = self.criterionGAN(pred_real, label_loss)\n",
    "\n",
    "    ### Generator loss (Fake Passability Loss)        \n",
    "    pred_fake = discriminator_model.forward(torch.cat((input_label, fake_image), dim=1))  \n",
    "    \n",
    "    # label_loss=  .....the ground truth to compute the generate loss to teach it to generate fake images......\n",
    "    \n",
    "    loss_G_GAN = self.criterionGAN(pred_fake, label_loss)\n",
    "    \n",
    "\n",
    "    ### Feature Matching Loss\n",
    "    loss_G_GAN_Feat = 0\n",
    "    feat_weights = 4.0 / (opt.n_layers_D + 1)\n",
    "    D_weights = 1.0 / opt.num_D\n",
    "    for i in range(opt.num_D):\n",
    "        for j in range(len(pred_fake[i])-1):\n",
    "            loss_G_GAN_Feat += D_weights * feat_weights * \\\n",
    "                Feat_Match_loss(pred_fake[i][j], pred_real[i][j].detach()) * opt.lambda_feat\n",
    "            \n",
    "    \n",
    "\n",
    "    ### VGG Loss\n",
    "    loss_G_VGG = 0\n",
    "    \n",
    "    input1, input2 = # ...... the inputs of to calculate the VGG loss using the L1 function to ensure VGG classifies fake and real images equally .........\n",
    "    \n",
    "    \n",
    "    loss_G_VGG = VGG_loss(fake_image, real_image) * opt.lambda_feat\n",
    "    \n",
    "    \n",
    "    # set the flags based on if the VGG and the Feature Matching Losses are used\n",
    "    flags = (True, not opt.no_ganFeat_loss, not opt.no_vgg_loss, True, True)\n",
    "    \n",
    "    # only return the fake_B image if necessary to save BW\n",
    "    losses_to_return=[[l for (l,f) in zip((loss_G_GAN, loss_G_GAN_Feat, loss_G_VGG, loss_D_real, loss_D_fake),flags) if f], None if not infer else fake_image]\n",
    "    \n",
    "\n",
    "    return losses_to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference Ability\n",
    "\n",
    "As it can be seen the inference ability of the network,\n",
    "\n",
    "Either they run the inference for real, or they just load a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amld",
   "language": "python",
   "name": "amld"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
