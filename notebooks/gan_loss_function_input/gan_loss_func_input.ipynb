{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Teaching Objective\n",
    "\n",
    "The aim of this notebook is to show the participants how the ***GAN loss*** are computed and what are the ***inputs*** of the models which implicitely affect the losses. \n",
    "\n",
    "Compared to the other notebooks we are introducing VGG and Feature Matching Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "\n",
    "- code GAN_loss\n",
    "- clean imports\n",
    "- debug training loop\n",
    "- Explain LSGAN loss \n",
    "- Explain VGG loss \n",
    "- Explain Feature matching loss part\n",
    "- make sure information written is correct and flow is good\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary package imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "sys.path.append('../../src/pix2pixHD')\n",
    "from src.pix2pixHD.models.pix2pixHD_model import Pix2PixHDModel\n",
    "from src.pix2pixHD.util.image_pool import ImagePool\n",
    "from src.pix2pixHD.models.base_model import BaseModel\n",
    "from src.pix2pixHD.models import networks\n",
    "import src.config.train_opt_notebook as opt\n",
    "\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "from torch.autograd import Variable\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dir_name = '../../src/GANcing/'\n",
    "pix2pixhd_dir = os.path.join(dir_name, '../pix2pixHD/')\n",
    "sys.path.append(pix2pixhd_dir)\n",
    "sys.path.append(os.path.join(dir_name, '../..'))\n",
    "sys.path.append(os.path.join(dir_name, '../utils'))\n",
    "\n",
    "\n",
    "from data.data_loader import CreateDataLoader\n",
    "from models.models import create_model\n",
    "import util.util as util\n",
    "# from util.visualizer import Visualizer\n",
    "from torch_utils import get_torch_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass Pix2Pix\n",
    "\n",
    "The aim of this notebook is to show how the ***GAN loss*** are computed and what are the ***inputs*** of the models involved in the training. \n",
    "\n",
    "VGG and Feature Matching Loss are introduced and their function is explained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before initializing the model class, the losses used to train GAN are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GAN_loss(use_lsgan=not opt.no_lsgan, tensor=torch.Tensor, gpu_ids=opt.gpu_ids):\n",
    "    \n",
    "    return networks.GANLoss(use_lsgan=use_lsgan, tensor=tensor, gpu_ids=gpu_ids)\n",
    "\n",
    "\n",
    "def VGG_loss(x,y):\n",
    "\n",
    "    #loss criteria\n",
    "    criterion = nn.L1Loss()\n",
    "\n",
    "    # The number of weights decrease gradually as the network becomes deeper: it mirrors the VGG network structure.\n",
    "    weights = [1.0/32, 1.0/16, 1.0/8, 1.0/4, 1.0]\n",
    "\n",
    "    # The features for different layers of the VGG are computed using the real and fake images\n",
    "    x_vgg, y_vgg = networks.Vgg19(x,y)\n",
    "\n",
    "    loss = 0\n",
    "    for i in range(len(x_vgg)):\n",
    "        loss += weights[i] * criterion(x_vgg[i], y_vgg[i].detach())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def Feat_Match_loss():\n",
    "\n",
    "    criterion=torch.nn.L1Loss()\n",
    "\n",
    "    return criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the model class is initialized.\n",
    "\n",
    "First, the generator (``self.net_G``) and the discriminator (``self.net_D``) models are initialized in the ``initialize`` function:\n",
    "\n",
    "Among the config variables which are used to define the network, some which are worth noticing are:\n",
    "\n",
    "- ``netG_input_nc`` is the number of classes which are fed in the generator model. In other words it is the number of coordinates to define a pose. ``netG_input_nc`` is the number of channels which correspond to the number of joints. The poses are fed as images with ``netG_input_nc`` number of channels.\n",
    "- ``netD_input_nc`` is the number of input classes going into the discriminator which are the classes defining a pose and those defining the image. i.e the 3 RGB channels are added to the input.\n",
    "\n",
    "Second, the forward pass is defined in ``forward``:\n",
    "\n",
    "The forward pass is the essential one which allows the network to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gancing(BaseModel):\n",
    "    \n",
    "    def initialize(self, opt):\n",
    "        BaseModel.initialize(self, opt)\n",
    "        if opt.resize_or_crop != 'none' or not opt.isTrain: # when training at full res this causes OOM\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "        self.isTrain = opt.isTrain\n",
    "\n",
    "        ######check for gpu\n",
    "        self.gpu_ids = opt.gpu_ids\n",
    "        if len(self.gpu_ids) > 0:\n",
    "            self.device = torch.device('cuda')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "            \n",
    "        \n",
    "        input_nc = opt.label_nc if opt.label_nc != 0 else opt.input_nc\n",
    "\n",
    "        # Generator network\n",
    "        netG_input_nc = input_nc        \n",
    "        self.netG = networks.define_G(netG_input_nc, opt.output_nc, opt.ngf, opt.netG, \n",
    "                                      opt.n_downsample_global, opt.n_blocks_global, opt.n_local_enhancers, \n",
    "                                      opt.n_blocks_local, opt.norm, gpu_ids=self.gpu_ids)        \n",
    "\n",
    "        # Discriminator network\n",
    "        if self.isTrain:\n",
    "            use_sigmoid = opt.no_lsgan\n",
    "            netD_input_nc = input_nc + opt.output_nc\n",
    "            self.netD = networks.define_D(netD_input_nc, opt.ndf, opt.n_layers_D, opt.norm, use_sigmoid, \n",
    "                                          opt.num_D, not opt.no_ganFeat_loss, gpu_ids=self.gpu_ids)\n",
    "\n",
    "\n",
    "        # load networks\n",
    "        if not self.isTrain or opt.continue_train or opt.load_pretrain:\n",
    "            pretrained_path = '' if not self.isTrain else opt.load_pretrain\n",
    "            self.load_network(self.netG, 'G', opt.which_epoch, pretrained_path)            \n",
    "            if self.isTrain:\n",
    "                self.load_network(self.netD, 'D', opt.which_epoch, pretrained_path)         \n",
    "\n",
    "        # set loss functions and optimizers\n",
    "        if self.isTrain:            \n",
    "            \n",
    "            # Names so we can breakout loss\n",
    "            self.loss_names = ['G_GAN','G_GAN_Feat','G_VGG','D_real', 'D_fake']\n",
    "\n",
    "            # initialize optimizers\n",
    "            params = list(self.netG.parameters())\n",
    "            self.optimizer_G = torch.optim.Adam(params, lr=opt.lr, betas=(opt.beta1, 0.999))                            \n",
    "\n",
    "            # optimizer D                        \n",
    "            params = list(self.netD.parameters())    \n",
    "            self.optimizer_D = torch.optim.Adam(params, lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "            \n",
    "    \n",
    "    def encode_input(self, label_map, real_image, infer=False): \n",
    "    \n",
    "        # create one-hot vector for the pose\n",
    "        size = label_map.size()\n",
    "        oneHot_size = (size[0], opt.label_nc, size[2], size[3])\n",
    "        input_label = torch.FloatTensor(torch.Size(oneHot_size)).zero_().to(self.device)\n",
    "        input_label = input_label.scatter_(1, label_map.data.long().to(self.device), 1.0)\n",
    "        if opt.data_type == 16:\n",
    "            input_label = input_label.half()\n",
    "\n",
    "        input_label = Variable(input_label, volatile=infer)\n",
    "\n",
    "        # real images for training\n",
    "        if real_image is not None:\n",
    "            real_image = Variable(real_image.data.to(self.device))\n",
    "\n",
    "        return input_label, real_image\n",
    "\n",
    "    def forward(self, label, image, infer=False):\n",
    "        \n",
    "        \n",
    "        # Encode Inputs\n",
    "        input_label, real_image = self.encode_input(label, image)  \n",
    "\n",
    "        # Fake Generation\n",
    "        fake_image = self.netG.forward(input_label.float())\n",
    "        \n",
    "\n",
    "        ### Detection\n",
    "\n",
    "        #predict if the real images as False or True      \n",
    "        pred_real = self.netD.forward(torch.cat((input_label, real_image), dim=1))\n",
    "        \n",
    "        #predict if the fake image as False or True\n",
    "        pred_fake = self.netD.forward(torch.cat((input_label, fake_image), dim=1)) \n",
    "\n",
    "        ##################### Dicriminator Loss Based on Fake Images #####################\n",
    "\n",
    "        # Try to find out by yourself the ground truth to compute the GAN loss for the discriminator for fake images\n",
    "        # GAN_label_Discriminator_Fake = ...\n",
    "        \n",
    "        loss_D_fake = GAN_loss(pred_fake, GAN_label_Discriminator_Fake)  \n",
    "\n",
    "        ##################### Dicriminator Loss Based on Real Images #####################\n",
    "        \n",
    "        # Try to find out by yourself the ground truth to compute the GAN loss for the discriminator for real images\n",
    "        # GAN_label_Discriminator_Real =  ... \n",
    "\n",
    "        loss_D_real = GAN_loss(pred_real, GAN_label_Discriminator_Real)\n",
    "\n",
    "        ##################### Generator Loss Based #####################       \n",
    "        \n",
    "        # Try to find out by yourself the ground truth to compute the GAN loss for the generator\n",
    "        # GAN_label_Generator=  ...\n",
    "\n",
    "        loss_G_GAN = GAN_loss(pred_fake, GAN_label_Generator)  \n",
    "                   \n",
    "        \n",
    "        ##################### Feature Matching Loss ##################### \n",
    "        \n",
    "        loss_G_GAN_Feat = 0\n",
    "        feat_weights = 4.0 / (opt.n_layers_D + 1)\n",
    "        D_weights = 1.0 / opt.num_D\n",
    "        for i in range(opt.num_D):\n",
    "            for j in range(len(pred_fake[i])-1):\n",
    "                loss_G_GAN_Feat += D_weights * feat_weights * \\\n",
    "                    Feat_Match_loss(pred_fake[i][j], pred_real[i][j].detach()) * opt.lambda_feat\n",
    "\n",
    "\n",
    "        ##################### VGG Loss ##################### \n",
    "        loss_G_VGG = 0\n",
    "\n",
    "        # Try to find out by yourself the inputs needed to calculate the VGG loss using the L1 function to ensure VGG classifies fake and real images equally\n",
    "        # input1, input2 = ... , ...\n",
    "\n",
    "        loss_G_VGG = VGG_loss(input1, input2) * opt.lambda_feat\n",
    "\n",
    "\n",
    "        # only return the fake_B image if necessary to save BW\n",
    "        losses_to_return=[[loss_G_GAN, loss_G_GAN_Feat, loss_G_VGG, loss_D_real, loss_D_fake], None if not infer else fake_image]\n",
    "\n",
    "\n",
    "        return losses_to_return\n",
    "\n",
    "    def inference(self, label):\n",
    "        # Encode Inputs\n",
    "        input_label, _ = self.encode_input(Variable(label), infer=True) \n",
    "\n",
    "        # Fake Generation\n",
    "    \n",
    "        fake_image = self.netG.forward(input_label)\n",
    "        return fake_image\n",
    "\n",
    "\n",
    "    def save(self, which_epoch):\n",
    "        self.save_network(self.netG, 'G', which_epoch, self.gpu_ids)\n",
    "        self.save_network(self.netD, 'D', which_epoch, self.gpu_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop and Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some explaination..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pose2vid(target_dir, run_name, temporal_smoothing=False):\n",
    "    import src.config.train_opt_notebook as opt\n",
    "\n",
    "    opt = update_opt(opt, target_dir, run_name, temporal_smoothing)\n",
    "\n",
    "    iter_path = os.path.join(opt.checkpoints_dir, opt.name, 'iter.json')\n",
    "    data_loader = CreateDataLoader(opt)\n",
    "    dataset = data_loader.load_data()\n",
    "    dataset_size = len(data_loader)\n",
    "    print('#training images = %d' % dataset_size)\n",
    "\n",
    "    if opt.load_pretrain != '':\n",
    "        with open(iter_path, 'r') as f:\n",
    "            iter_json = json.load(f)\n",
    "    else:\n",
    "        iter_json = {'start_epoch': 1, 'epoch_iter': 0}\n",
    "\n",
    "    start_epoch = iter_json['start_epoch']\n",
    "    epoch_iter = iter_json['epoch_iter']\n",
    "    total_steps = (start_epoch - 1) * dataset_size + epoch_iter\n",
    "    display_delta = total_steps % opt.display_freq\n",
    "    print_delta = total_steps % opt.print_freq\n",
    "    save_delta = total_steps % opt.save_latest_freq\n",
    "\n",
    "    model = create_model(opt)\n",
    "    model = model.to(device)\n",
    "#     visualizer = Visualizer(opt)\n",
    "\n",
    "    for epoch in range(start_epoch, opt.niter + opt.niter_decay + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        if epoch != start_epoch:\n",
    "            epoch_iter = epoch_iter % dataset_size\n",
    "        for i, data in enumerate(dataset, start=epoch_iter):\n",
    "            iter_start_time = time.time()\n",
    "            total_steps += opt.batchSize\n",
    "            epoch_iter += opt.batchSize\n",
    "\n",
    "            # whether to collect output images\n",
    "            save_fake = total_steps % opt.display_freq == display_delta\n",
    "\n",
    "            ############## Forward Pass ######################\n",
    "            if temporal_smoothing:\n",
    "                losses, generated = model(Variable(data['label']), Variable(data['inst']),\n",
    "                                          Variable(data['image']), Variable(data['feat']),\n",
    "                                          Variable(data['previous_label']), Variable(data['previous_image']), infer=save_fake)\n",
    "            else:\n",
    "                losses, generated = model(Variable(data['label']), Variable(data['inst']),\n",
    "                                        Variable(data['image']), Variable(data['feat']), infer=save_fake)\n",
    "\n",
    "            # sum per device losses\n",
    "            losses = [torch.mean(x) if not isinstance(x, int) else x for x in losses]\n",
    "            loss_dict = dict(zip(model.loss_names, losses))\n",
    "\n",
    "            # calculate final loss scalar\n",
    "            loss_D = (loss_dict['D_fake'] + loss_dict['D_real']) * 0.5\n",
    "            loss_G = loss_dict['G_GAN'] + loss_dict.get('G_GAN_Feat', 0) + loss_dict.get('G_VGG', 0)\n",
    "\n",
    "            ############### Backward Pass ####################\n",
    "            # update generator weights\n",
    "            model.optimizer_G.zero_grad()\n",
    "            loss_G.backward(retain_graph=True)\n",
    "            model.optimizer_G.step()\n",
    "\n",
    "            # update discriminator weights\n",
    "            model.optimizer_D.zero_grad()\n",
    "            loss_D.backward()\n",
    "            model.optimizer_D.step()\n",
    "\n",
    "\n",
    "            ############## Display results and errors ##########\n",
    "\n",
    "            print(f\"Epoch {epoch} batch {i}:\")\n",
    "            print(f\"loss_D: {loss_D}, loss_G: {loss_G}\")\n",
    "            print(f\"loss_D_fake: {loss_dict['D_fake']}, loss_D_real: {loss_dict['D_real']}\")\n",
    "            print(f\"loss_G_GAN {loss_dict['G_GAN']}, loss_G_GAN_Feat: {loss_dict.get('G_GAN_Feat', 0)}, loss_G_VGG: {loss_dict.get('G_VGG', 0)}\\n\")\n",
    "\n",
    "            ### print out errors\n",
    "            if total_steps % opt.print_freq == print_delta:\n",
    "                errors = {k: v.item() if not isinstance(v, int) else v for k, v in loss_dict.items()}\n",
    "                # errors = {k: v.data[0] if not isinstance(v, int) else v for k, v in loss_dict.items()}\n",
    "                t = (time.time() - iter_start_time) / opt.batchSize\n",
    "#                 visualizer.print_current_errors(epoch, epoch_iter, errors, t)\n",
    "#                 visualizer.plot_current_errors(errors, total_steps)\n",
    "\n",
    "            ### display output images\n",
    "            if save_fake:\n",
    "                visuals = OrderedDict([('input_label', util.tensor2label(data['label'][0], opt.label_nc)),\n",
    "                                       ('synthesized_image', util.tensor2im(generated.data[0])),\n",
    "                                       ('real_image', util.tensor2im(data['image'][0]))])\n",
    "#                 visualizer.display_current_results(visuals, epoch, total_steps)\n",
    "\n",
    "            ### save latest model\n",
    "            if total_steps % opt.save_latest_freq == save_delta:\n",
    "                print('saving the latest model (epoch %d, total_steps %d)' % (epoch, total_steps))\n",
    "                model.save('latest')\n",
    "                iter_json['start_epoch'] = epoch\n",
    "                iter_json['epoch_iter'] = epoch_iter\n",
    "                with open(iter_path, 'w') as f:\n",
    "                    json.dump(iter_json, f)\n",
    "\n",
    "            if epoch_iter >= dataset_size:\n",
    "                break\n",
    "\n",
    "        # end of epoch\n",
    "        print('End of epoch %d / %d \\t Time Taken: %d sec' %\n",
    "              (epoch, opt.niter + opt.niter_decay, time.time() - epoch_start_time))\n",
    "\n",
    "        ### save model for this epoch\n",
    "        if epoch % opt.save_epoch_freq == 0:\n",
    "            print('saving the model at the end of epoch %d, iters %d' % (epoch, total_steps))\n",
    "            model.save('latest')\n",
    "            model.save(epoch)\n",
    "            iter_json['start_epoch'] = epoch + 1\n",
    "            iter_json['epoch_iter'] = 0\n",
    "            with open(iter_path, 'w') as f:\n",
    "                json.dump(iter_json, f)\n",
    "\n",
    "        ### instead of only training the local enhancer, train the entire network after certain iterations\n",
    "        if (opt.niter_fix_global != 0) and (epoch == opt.niter_fix_global):\n",
    "            model.update_fixed_params()\n",
    "\n",
    "        ### linearly decay learning rate after certain iterations\n",
    "        if epoch > opt.niter:\n",
    "            model.update_learning_rate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_opt(opt, target_dir, run_name, temporal_smoothing):\n",
    "    opt.dataroot = os.path.join(target_dir, 'train')\n",
    "    opt.name = run_name\n",
    "    if os.path.isdir(os.path.join(dir_name, \"../../checkpoints\", run_name)):\n",
    "        print(\"Run already exists, will try to resume training\")\n",
    "        opt.load_pretrain = os.path.join(dir_name, \"../../checkpoints\", run_name)\n",
    "\n",
    "    if device == torch.device('cpu'):\n",
    "        opt.gpu_ids = []\n",
    "\n",
    "    opt.temporal_smoothing = temporal_smoothing\n",
    "\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomDatasetDataLoader\n",
      "dataset [AlignedDataset] was created\n",
      "#training images = 100\n",
      "GlobalGenerator(\n",
      "  (model): Sequential(\n",
      "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (1): Conv2d(18, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (3): ReLU(inplace)\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (6): ReLU(inplace)\n",
      "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (9): ReLU(inplace)\n",
      "    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (11): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (12): ReLU(inplace)\n",
      "    (13): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (14): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (15): ReLU(inplace)\n",
      "    (16): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (17): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (18): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (19): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (20): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (21): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (22): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (23): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (24): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace)\n",
      "        (4): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (5): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (6): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (25): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (26): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (27): ReLU(inplace)\n",
      "    (28): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (29): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (30): ReLU(inplace)\n",
      "    (31): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (32): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (33): ReLU(inplace)\n",
      "    (34): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (35): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (36): ReLU(inplace)\n",
      "    (37): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (38): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (39): Tanh()\n",
      "  )\n",
      ")\n",
      "MultiscaleDiscriminator(\n",
      "  (scale0_layer0): Sequential(\n",
      "    (0): Conv2d(21, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
      "  )\n",
      "  (scale0_layer1): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
      "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace)\n",
      "  )\n",
      "  (scale0_layer2): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
      "    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace)\n",
      "  )\n",
      "  (scale0_layer3): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
      "    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace)\n",
      "  )\n",
      "  (scale0_layer4): Sequential(\n",
      "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
      "  )\n",
      "  (scale1_layer0): Sequential(\n",
      "    (0): Conv2d(21, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
      "  )\n",
      "  (scale1_layer1): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
      "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace)\n",
      "  )\n",
      "  (scale1_layer2): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
      "    (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace)\n",
      "  )\n",
      "  (scale1_layer3): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
      "    (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (2): LeakyReLU(negative_slope=0.2, inplace)\n",
      "  )\n",
      "  (scale1_layer4): Sequential(\n",
      "    (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
      "  )\n",
      "  (downsample): AvgPool2d(kernel_size=3, stride=2, padding=[1, 1])\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /Users/gianlucamancini/.torch/models/vgg19-dcbb9e9d.pth\n",
      "100%|██████████| 574673361/574673361 [02:19<00:00, 4121658.56it/s]\n"
     ]
    }
   ],
   "source": [
    "device = get_torch_device()\n",
    "train_pose2vid('../../data/targets/gianluca', 'trial_workshop', temporal_smoothing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GancingModel(Pix2PixHDModel):\n",
    "    \n",
    "    def __init__(self, opt):\n",
    "        \n",
    "        Pix2PixHDModel.initialize(opt)\n",
    "    \n",
    "        netG_input_nc = opt.label_nc\n",
    "        self.generator_model=networks.define_G(netG_input_nc, opt.output_nc, opt.ngf, opt.netG, \n",
    "                                              opt.n_downsample_global, opt.n_blocks_global, opt.n_local_enhancers, \n",
    "                                              opt.n_blocks_local, opt.norm, gpu_ids=opt.gpu_ids)   \n",
    "\n",
    "        use_sigmoid = opt.no_lsgan\n",
    "        netD_input_nc = netG_input_nc + opt.output_nc\n",
    "        self.discriminator_model=networks.define_D(netD_input_nc, opt.ndf, opt.n_layers_D, opt.norm, use_sigmoid, \n",
    "                                                  opt.num_D, not opt.no_ganFeat_loss, gpu_ids=opt.gpu_ids)\n",
    "    \n",
    "    def encode_input(self, label_map, real_image, infer=False): \n",
    "    \n",
    "        # create one-hot vector for the pose\n",
    "        size = label_map.size()\n",
    "        oneHot_size = (size[0], opt.label_nc, size[2], size[3])\n",
    "        input_label = torch.FloatTensor(torch.Size(oneHot_size)).zero_().to(self.device)\n",
    "        input_label = input_label.scatter_(1, label_map.data.long().to(self.device), 1.0)\n",
    "        if opt.data_type == 16:\n",
    "            input_label = input_label.half()\n",
    "\n",
    "        input_label = Variable(input_label, volatile=infer)\n",
    "\n",
    "        # real images for training\n",
    "        if real_image is not None:\n",
    "            real_image = Variable(real_image.data.to(self.device))\n",
    "\n",
    "\n",
    "        return input_label, real_image\n",
    "        \n",
    "    def forward(self, label, image, feat, infer=False):\n",
    "        \n",
    "        input_label, real_image = self.encode_input(label, image)\n",
    "\n",
    "        input_concat = input_label\n",
    "\n",
    "        ### Fake Generation\n",
    "        fake_image = self.generator_model.forward(input_concat.float())\n",
    "\n",
    "        ### Fake Detection\n",
    "\n",
    "        #concatenate the condition input_label used to generate the image and the fake image \n",
    "        input_concat = torch.cat((input_label, fake_image.detach()), dim=1) \n",
    "\n",
    "        #adapt the image dimension\n",
    "        fake_query = ImagePool(opt.pool_size).query(input_concat)\n",
    "\n",
    "        #predict if the generate image as False or True\n",
    "        pred_fake_pool = self.discriminator_model.forward(fake_query)\n",
    "\n",
    "        #predict if the real images as False or True      \n",
    "        pred_real = self.discriminator_model.forward(input_label, real_image)\n",
    "        \n",
    "        #predict if the fake image as False or True\n",
    "        pred_fake = self.discriminator_model.forward(torch.cat((input_label, fake_image), dim=1)) \n",
    "\n",
    "        ##################### Dicriminator Loss Based on Fake Images #####################\n",
    "\n",
    "        # Try to find out by yourself the ground truth to compute the GAN loss for the discriminator for fake images\n",
    "        # GAN_label_Discriminator = ...\n",
    "\n",
    "        loss_D_fake = self.criterionGAN(pred_fake_pool, GAN_label_Discriminator)      \n",
    "\n",
    "        ##################### Dicriminator Loss Based on Real Images #####################\n",
    "        \n",
    "        # Try to find out by yourself the ground truth to compute the GAN loss for the discriminator for real images\n",
    "        # GAN_label_Discriminator =  ... \n",
    "\n",
    "        loss_D_real = self.criterionGAN(pred_real, GAN_label_Discriminator)\n",
    "\n",
    "        ##################### Generator Loss Based #####################       \n",
    "        \n",
    "        # Try to find out by yourself the ground truth to compute the GAN loss for the generator\n",
    "        # label_loss=  ...\n",
    "\n",
    "        loss_G_GAN = self.criterionGAN(pred_fake, label_loss)\n",
    "\n",
    "\n",
    "        ##################### Feature Matching Loss ##################### \n",
    "        \n",
    "        loss_G_GAN_Feat = 0\n",
    "        feat_weights = 4.0 / (opt.n_layers_D + 1)\n",
    "        D_weights = 1.0 / opt.num_D\n",
    "        for i in range(opt.num_D):\n",
    "            for j in range(len(pred_fake[i])-1):\n",
    "                loss_G_GAN_Feat += D_weights * feat_weights * \\\n",
    "                    Feat_Match_loss(pred_fake[i][j], pred_real[i][j].detach()) * opt.lambda_feat\n",
    "\n",
    "\n",
    "\n",
    "        ##################### VGG Loss ##################### \n",
    "        loss_G_VGG = 0\n",
    "\n",
    "        # Try to find out by yourself the inputs needed to calculate the VGG loss using the L1 function to ensure VGG classifies fake and real images equally\n",
    "        # input1, input2 = ... , ...\n",
    "\n",
    "        loss_G_VGG = VGG_loss(input1, input2) * opt.lambda_feat\n",
    "\n",
    "\n",
    "        # set the flags based on if the VGG and the Feature Matching Losses are used\n",
    "        flags = (True, not opt.no_ganFeat_loss, not opt.no_vgg_loss, True, True)\n",
    "\n",
    "        # only return the fake_B image if necessary to save BW\n",
    "        losses_to_return=[[l for (l,f) in zip((loss_G_GAN, loss_G_GAN_Feat, loss_G_VGG, loss_D_real, loss_D_fake),flags) if f], None if not infer else fake_image]\n",
    "\n",
    "\n",
    "        return losses_to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis of original function\n",
    "\n",
    "In the forward pass the different losses used for the training are defined. The most important components of the forward pass are the following:\n",
    "\n",
    "Networks to be trained to generate and discriminate\n",
    "1. ``self.netG``: the generator network\n",
    "2. ``self.netD``: the discriminator network\n",
    "\n",
    "Loss Discriminator\n",
    "1. ``loss_D_fake``, discriminator loss from discriminating fake generated images: -log(1-D(G(z)))\n",
    "2. ``loss_D_real``, discriminator loss from discriminating real images sampled from data: -log(D(x))\n",
    "\n",
    "Loss Generator\n",
    "1. ``loss_G_Gan``, generator loss from the \"Fake Pass Loss\" which label the fake images as correct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Original Function***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, label, inst, image, feat, infer=False):\n",
    "    # Encode Inputs\n",
    "    input_label, inst_map, real_image, feat_map = self.encode_input(label, inst, image, feat)  \n",
    "\n",
    "    # Fake Generation\n",
    "    if self.use_features:\n",
    "        if not self.opt.load_features:\n",
    "            feat_map = self.netE.forward(real_image, inst_map)                     \n",
    "        input_concat = torch.cat((input_label, feat_map), dim=1)                        \n",
    "    else:\n",
    "        input_concat = input_label\n",
    "    # TODO----------------------#    \n",
    "    fake_image = self.netG.forward(input_concat.float())\n",
    "\n",
    "    # Fake Detection and Loss\n",
    "    pred_fake_pool = self.discriminate(input_label, fake_image, use_pool=True)\n",
    "    loss_D_fake = self.criterionGAN(pred_fake_pool, False)        \n",
    "\n",
    "    # Real Detection and Loss        \n",
    "    pred_real = self.discriminate(input_label, real_image)\n",
    "    loss_D_real = self.criterionGAN(pred_real, True)\n",
    "\n",
    "    # GAN loss (Fake Passability Loss)        \n",
    "    pred_fake = self.netD.forward(torch.cat((input_label, fake_image), dim=1))        \n",
    "    loss_G_GAN = self.criterionGAN(pred_fake, True)               \n",
    "\n",
    "    # GAN feature matching loss\n",
    "    loss_G_GAN_Feat = 0\n",
    "    if not self.opt.no_ganFeat_loss:\n",
    "        feat_weights = 4.0 / (self.opt.n_layers_D + 1)\n",
    "        D_weights = 1.0 / self.opt.num_D\n",
    "        for i in range(self.opt.num_D):\n",
    "            for j in range(len(pred_fake[i])-1):\n",
    "                loss_G_GAN_Feat += D_weights * feat_weights * \\\n",
    "                    self.criterionFeat(pred_fake[i][j], pred_real[i][j].detach()) * self.opt.lambda_feat\n",
    "\n",
    "    # VGG feature matching loss\n",
    "    loss_G_VGG = 0\n",
    "    if not self.opt.no_vgg_loss:\n",
    "        loss_G_VGG = self.criterionVGG(fake_image, real_image) * self.opt.lambda_feat\n",
    "\n",
    "    # Only return the fake_B image if necessary to save BW\n",
    "    return [ self.loss_filter( loss_G_GAN, loss_G_GAN_Feat, loss_G_VGG, loss_D_real, loss_D_fake ), None if not infer else fake_image ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the notebook of easier comprehension, the functions have been simplified and commented.\n",
    "\n",
    "``forward`` function modifications:\n",
    "\n",
    "-``netE``: not in use\n",
    "\n",
    "-``nwtG`` and ``netD`` become functions themselves which are initialized before\n",
    "\n",
    "-``self.encode_input`` become a separate function to explain what goes into the model and is placed outside the forward to teach the audience what goes in the forward path and needs to be encoded. The encoding part itself is probably irrelevant, but a independent function has to be created if an example of forward pass has to be created.\n",
    "\n",
    "The forward pass is the most interesting one, here we can show it detached from the class itself, and then when we show the training loop, we highlight where the model calls it.\n",
    "\n",
    "\n",
    "\n",
    "*Ideas to clean up the function more*:\n",
    "\n",
    "-``criterionGAN`` becomes a function\n",
    "\n",
    "-condition of training the gan based on the generator and discriminator features' neurons.\n",
    "\n",
    "-condition of training the gan using VGG to recognize images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amld",
   "language": "python",
   "name": "amld"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
